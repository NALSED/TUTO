# 🤖 Installation et Configuration  d''un model Ai en Local. 🤖

---

##  1️⃣ 🥼 Labo + Solution 🥼

* #### `Labo`
    * #### RTX 4080super VRAM 16GB
    * #### i9 13900kf
    * #### 64 GB ddr5 4800

* #### `Solution`
    * #### CUDA (Compute Unified Device Architecture) : CUDA transforme la carte graphique NVIDIA en superprocesseur parallèle capable d’effectuer des milliers d’opérations en même temps.
    * #### Ollama Ollama est une plateforme et un outil conçu pour faciliter l’utilisation des grands modèles de langage (LLM) en local sur ton ordinateur.
    * #### DeepSeek Coder 6.7B : est un modèle de langage spécialisé dans la génération de code informatique, la configuration système, l’administration réseau, le DevOps.
    * #### OpenWebUI : interface web open-source qui permet d’interagir facilement avec des modèles de langage locaux (LLM) via un navigateur.

  ---
## 2️⃣ 💾 installation 💾

* #### 1) `CUDA`
#### [Télécharger](https://developer.nvidia.com/cuda-downloads) et installer CUDA.
#### On peux utiliser ces commande pour voir la carte et la version CUDA installée.
      nvidia-smi
      nvcc --version

* #### 2) `Ollama`
[SOURCE](https://www.youtube.com/@AdrienLinuxtricks/search?query=Ollama)
#### [Télécharger]() et installer Ollama.
#### Si besoin changer le chemin d'installation des models
   * #### 1) Ouvrir  Ollama cliquer en haut à gauche
<img width="795" height="629" alt="image" src="https://github.com/user-attachments/assets/3aa0c26f-c36e-4404-b63e-0d6f77dec341" />
